{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment :- WAP to train and evaluate a convolutional neural network using Keras Library to\n",
    "classify MNIST fashion dataset. Demonstrate the effect of filter size, regularization,\n",
    "batch size and optimization algorithm on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture:\n",
    "1.Input Layer:\n",
    "\n",
    "• The model accepts input images of size 28x28x1, where 28x28 is the image dimension and 1 is the number of channels (grayscale).\n",
    "\n",
    "2.First Convolutional Layer:\n",
    "\n",
    "• Conv2D: The first convolutional layer has 32 filters, each of size (3x3), with the activation function set to ReLU.\n",
    "\n",
    "• Input Shape: The input shape for this layer is (28, 28, 1), meaning the image is 28x28 pixels with 1 color channel (grayscale).\n",
    "\n",
    "• Padding: It uses valid padding, which means no padding is applied to the image, so the spatial dimensions will shrink after the convolution operation.\n",
    "\n",
    "• After applying this convolution, the output shape will be (26, 26, 32) because a 3x3 filter reduces the image size by 2 in each dimension.\n",
    "\n",
    "3.First MaxPooling Layer:\n",
    "\n",
    "• MaxPooling2D: This layer performs a 2x2 max pooling operation, which downsamples the feature map by taking the maximum value in each 2x2 patch.\n",
    "\n",
    "• After pooling, the spatial dimensions are halved, so the output shape is reduced to (13, 13, 32).\n",
    "\n",
    "4.Second Convolutional Layer:\n",
    "\n",
    "• Conv2D: The second convolutional layer has 64 filters, each of size (3x3), with ReLU activation.\n",
    "\n",
    "• This layer takes the output from the previous max-pooling layer, which has the shape (13, 13, 32).\n",
    "\n",
    "• After applying the convolution, the output shape becomes (11, 11, 64).\n",
    "\n",
    "5.Second MaxPooling Layer:\n",
    "\n",
    "• MaxPooling2D: Another 2x2 max pooling operation, which reduces the spatial dimensions by half again. The output shape is now (5, 5, 64).\n",
    "\n",
    "6.Third Convolutional Layer:\n",
    "\n",
    "• Conv2D: The third convolutional layer has 128 filters, each of size (3x3), with ReLU activation.\n",
    "\n",
    "• The input to this layer has the shape (5, 5, 64).\n",
    "\n",
    "• After applying the convolution, the output shape becomes (3, 3, 128).\n",
    "\n",
    "7.Third MaxPooling Layer:\n",
    "\n",
    "• MaxPooling2D: A final 2x2 max pooling operation. This reduces the spatial dimensions once more, and the output shape is now (1, 1, 128).\n",
    "\n",
    "8.Flatten Layer:\n",
    "\n",
    "• Flatten: This layer flattens the 3D feature map (1, 1, 128) into a 1D vector of size 128. Flattening is necessary before feeding the data into fully connected layers (Dense layers).\n",
    "\n",
    "9.Fully Connected Layer (Dense):\n",
    "\n",
    "• Dense: The first dense layer has 128 units and uses ReLU activation.\n",
    "\n",
    "This layer processes the flattened feature map and outputs a vector of size 128.\n",
    "\n",
    "10.Output Layer:\n",
    "\n",
    "• Dense: The output layer has 10 units (one for each class in the Fashion MNIST dataset) with a softmax activation function. This outputs a probability distribution over the 10 classes, and the class with the highest probability is chosen as the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 55ms/step - accuracy: 0.6876 - loss: 0.8506 - val_accuracy: 0.8134 - val_loss: 0.5050\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.8448 - loss: 0.4276 - val_accuracy: 0.8587 - val_loss: 0.3975\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - accuracy: 0.8677 - loss: 0.3560 - val_accuracy: 0.8659 - val_loss: 0.3652\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 53ms/step - accuracy: 0.8836 - loss: 0.3157 - val_accuracy: 0.8770 - val_loss: 0.3416\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 55ms/step - accuracy: 0.8929 - loss: 0.2891 - val_accuracy: 0.8822 - val_loss: 0.3218\n",
      "313/313 - 3s - 8ms/step - accuracy: 0.8822 - loss: 0.3218\n",
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 56ms/step - accuracy: 0.6956 - loss: 0.8356 - val_accuracy: 0.8343 - val_loss: 0.4462\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 0.8502 - loss: 0.4117 - val_accuracy: 0.8545 - val_loss: 0.3957\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 54ms/step - accuracy: 0.8704 - loss: 0.3479 - val_accuracy: 0.8701 - val_loss: 0.3544\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 57ms/step - accuracy: 0.8852 - loss: 0.3059 - val_accuracy: 0.8667 - val_loss: 0.3603\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 60ms/step - accuracy: 0.8937 - loss: 0.2829 - val_accuracy: 0.8840 - val_loss: 0.3193\n",
      "313/313 - 3s - 10ms/step - accuracy: 0.8840 - loss: 0.3193\n",
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 51ms/step - accuracy: 0.6506 - loss: 1.3217 - val_accuracy: 0.7795 - val_loss: 0.7673\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 59ms/step - accuracy: 0.7875 - loss: 0.7305 - val_accuracy: 0.7985 - val_loss: 0.6881\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 58ms/step - accuracy: 0.8003 - loss: 0.6741 - val_accuracy: 0.8057 - val_loss: 0.6621\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 56ms/step - accuracy: 0.8156 - loss: 0.6297 - val_accuracy: 0.8183 - val_loss: 0.6138\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 59ms/step - accuracy: 0.8221 - loss: 0.6013 - val_accuracy: 0.8054 - val_loss: 0.6404\n",
      "313/313 - 3s - 10ms/step - accuracy: 0.8054 - loss: 0.6404\n",
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 35ms/step - accuracy: 0.7179 - loss: 0.7718 - val_accuracy: 0.8278 - val_loss: 0.4658\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 35ms/step - accuracy: 0.8564 - loss: 0.3888 - val_accuracy: 0.8555 - val_loss: 0.3842\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 38ms/step - accuracy: 0.8773 - loss: 0.3287 - val_accuracy: 0.8682 - val_loss: 0.3452\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 37ms/step - accuracy: 0.8936 - loss: 0.2864 - val_accuracy: 0.8789 - val_loss: 0.3295\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 32ms/step - accuracy: 0.9049 - loss: 0.2549 - val_accuracy: 0.8866 - val_loss: 0.3080\n",
      "313/313 - 2s - 8ms/step - accuracy: 0.8866 - loss: 0.3080\n",
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 44ms/step - accuracy: 0.3556 - loss: 1.9126 - val_accuracy: 0.6666 - val_loss: 0.9046\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 60ms/step - accuracy: 0.7012 - loss: 0.8066 - val_accuracy: 0.7167 - val_loss: 0.7598\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 57ms/step - accuracy: 0.7455 - loss: 0.6852 - val_accuracy: 0.7423 - val_loss: 0.7184\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 58ms/step - accuracy: 0.7672 - loss: 0.6241 - val_accuracy: 0.7731 - val_loss: 0.6145\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 57ms/step - accuracy: 0.7822 - loss: 0.5844 - val_accuracy: 0.7831 - val_loss: 0.5885\n",
      "313/313 - 3s - 10ms/step - accuracy: 0.7831 - loss: 0.5885\n",
      "Config: {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 64, 'optimizer': 'adam'}, Test Accuracy: 0.8822\n",
      "Config: {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 64, 'optimizer': 'adam'}, Test Accuracy: 0.8840\n",
      "Config: {'filter_size': (3, 3), 'regularizer': <keras.src.regularizers.regularizers.L2 object at 0x00000280D5E5B110>, 'batch_size': 64, 'optimizer': 'adam'}, Test Accuracy: 0.8054\n",
      "Config: {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 32, 'optimizer': 'adam'}, Test Accuracy: 0.8866\n",
      "Config: {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 64, 'optimizer': 'sgd'}, Test Accuracy: 0.7831\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Reshape input to match CNN input format (28x28x1)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Step 2: Build a CNN model\n",
    "def build_cnn_model(filter_size=(3,3), regularizer=None, batch_size=64, optimizer='adam'):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add convolutional layer\n",
    "    model.add(layers.Conv2D(32, filter_size, activation='relu', input_shape=(28, 28, 1), kernel_regularizer=regularizer))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Pooling reduces spatial size\n",
    "    model.add(layers.Conv2D(64, filter_size, activation='relu', kernel_regularizer=regularizer))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Pooling reduces spatial size\n",
    "    model.add(layers.Conv2D(128, filter_size, activation='relu', kernel_regularizer=regularizer))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))  # Pooling reduces spatial size\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model with the given optimizer\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Train and evaluate the model for different configurations\n",
    "\n",
    "# Helper function to train and evaluate\n",
    "def train_evaluate_model(filter_size, regularizer, batch_size, optimizer):\n",
    "    model = build_cnn_model(filter_size, regularizer, batch_size, optimizer)\n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_data=(x_test, y_test), verbose=1)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "    return test_acc\n",
    "\n",
    "# Experiment with different configurations\n",
    "configurations = [\n",
    "    {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 64, 'optimizer': 'adam'},\n",
    "    {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 64, 'optimizer': 'adam'},\n",
    "    {'filter_size': (3, 3), 'regularizer': regularizers.l2(0.01), 'batch_size': 64, 'optimizer': 'adam'},\n",
    "    {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 32, 'optimizer': 'adam'},\n",
    "    {'filter_size': (3, 3), 'regularizer': None, 'batch_size': 64, 'optimizer': 'sgd'},\n",
    "]\n",
    "\n",
    "# Train and evaluate each configuration\n",
    "results = []\n",
    "\n",
    "for config in configurations:\n",
    "    acc = train_evaluate_model(config['filter_size'], config['regularizer'], config['batch_size'], config['optimizer'])\n",
    "    results.append((config, acc))\n",
    "\n",
    "# Print the results\n",
    "for config, acc in results:\n",
    "    print(f\"Config: {config}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Result:- validation accuracy increases over epochs, but it fluctuates compared to training accuracy. The model is generalizing well, with a slight increase in validation accuracy towards the end.\n",
    "\n",
    "If the validation accuracy were to drop drastically while the training accuracy kept increasing, it would indicate overfitting, but in this case, they track quite similarly, which is good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
